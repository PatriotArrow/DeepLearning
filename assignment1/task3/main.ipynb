{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b143962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b59ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor:\n",
    "    def __init__(self, alpha=0.01, max_iters=100000, threshold=1e-6, N=1000,hidden_layers=1, hidden_layer_size=3, seed=42):\n",
    "        self.alpha = alpha\n",
    "        self.max_iters = max_iters\n",
    "        self.threshold = threshold\n",
    "        self.N = N\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.hidden_layer_size = hidden_layer_size # not sure what to do with this. \n",
    "        self.seed = seed\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        self.trained = False #to be used in predict\n",
    "        \n",
    "\n",
    "\n",
    "    def sigmoid(self, t):\n",
    "        return 1/(1 + np.exp(-t))\n",
    "\n",
    "    #def relu(self, z):\n",
    "        #return np.maximum(0, z)\n",
    "\n",
    "    #def relu_derivative(self, z):\n",
    "        #return (z > 0).astype(float)\n",
    "    \n",
    "    def loss_function(self, y, y_cap):\n",
    "        epsilon = np.finfo(float).eps\n",
    "        y_cap = np.clip(y_cap, epsilon, 1 - epsilon)\n",
    "        return -(y * np.log(y_cap) + (1 - y) * np.log(1 - y_cap))\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        np.random.seed(self.seed)\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "\n",
    "        \n",
    "        m = X.shape[0] # samples\n",
    "        n = X.shape[1] # features\n",
    "\n",
    "        #init weights and bias\n",
    "        W1 = np.random.randn(self.hidden_layer_size,n) * 0.01 #random values with mean=0, stdev=0.01 ?\n",
    "        b1 = np.zeros((self.hidden_layer_size,1))\n",
    "\n",
    "        W2 = np. random.randn(1,self.hidden_layer_size) * 0.01 \n",
    "        b2 = np.zeros((1, 1))\n",
    "\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y).reshape(-1, 1)\n",
    "\n",
    "\n",
    "        stopping = False; loss_prev = 0; iteration = 0; acc = 0\n",
    "        \n",
    "\n",
    "        while not stopping:\n",
    "            #select random data point\n",
    "            \n",
    "            i = np.random.choice(np.arange(m))  # pick a random index\n",
    "            x = X[i].reshape(-1,1)\n",
    "            y = Y[i, 0]\n",
    "\n",
    "            #forward pass\n",
    "            a1 = self.sigmoid(W1 @ x + b1)\n",
    "            y_cap = (self.sigmoid(W2 @ a1 + b2)).item()\n",
    "            \n",
    "            \n",
    "            loss = self.loss_function(y, y_cap)\n",
    "            self.loss_history.append(float(loss))\n",
    "            \n",
    "            #back propagation\n",
    "            #output layer\n",
    "            deltaz = y_cap - y\n",
    "            delW2 = deltaz * a1.T\n",
    "            delb2 = deltaz\n",
    "            #hidden layer\n",
    "            deltaz1 = (W2.T * deltaz) * (a1 * (1 - a1))\n",
    "            delW1 = deltaz1 @ x.T    # gradient of hidden weights\n",
    "            delb1 = deltaz1           # gradient of hidden biases\n",
    "\n",
    "            #stochatic gradient descent\n",
    "            W1 = W1 - self.alpha * delW1\n",
    "            b1 = b1 - self.alpha * delb1\n",
    "            W2 = W2 - self.alpha * delW2\n",
    "            b2 = b2 - self.alpha * delb2\n",
    "\n",
    "            #to store accuracy per iteration\n",
    "            pred_i = 1 if y_cap >= 0.5 else 0\n",
    "            self.accuracy_history.append(1.0 if pred_i == y else 0.0)\n",
    "\n",
    "        \n",
    "            loss_prev = loss\n",
    "            iteration += 1\n",
    "            if iteration >= self.max_iters:\n",
    "                stopping = True\n",
    "\n",
    "        #set weights here\n",
    "        self.W1 = W1\n",
    "        self.b1 = b1\n",
    "        self.W2 = W2\n",
    "        self.b2 = b2\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self,x):\n",
    "        if (self.trained == False):\n",
    "            raise ValueError(\"Not trained uyet!\")\n",
    "        \n",
    "        #change here\n",
    "        a1 = self.sigmoid(self.W1 @ x.T + self.b1)\n",
    "        y_prob = self.sigmoid(self.W2 @ a1 + self.b2)\n",
    "\n",
    "        Y_pred = (y_prob >= 0.5).astype(int)\n",
    "        return Y_pred\n",
    "    \n",
    "    def score(self, x_test, y_test):\n",
    "        if (self.trained == False):\n",
    "            raise ValueError(\"Not trained uyet!\")        \n",
    "        \n",
    "        y_pred = self.predict(x_test)\n",
    "        #running into shape issues so trying this \n",
    "        y_pred = y_pred.flatten()\n",
    "        y_test = y_test.flatten()\n",
    "        correct = 0\n",
    "        size = len(y_test)\n",
    "\n",
    "        for i in range(size):\n",
    "            if (y_pred[i]==y_test[i]):\n",
    "                correct+=1\n",
    "\n",
    "        return correct/size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b6258c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#file_name = r\"../datasets/blobs600.csv\"\n",
    "file_name = r\"../datasets/circles500.csv\"\n",
    "df1 = pd.read_csv(file_name)\n",
    "Y = df1['Class'].values\n",
    "del df1['Class']  \n",
    "X = df1.values     \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ece07b-ea39-4c52-8ea6-df9ca81a2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def data_split(X, Y, seed=42):\n",
    "    Y = np.array(Y).reshape(-1,1)\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split( X,Y,test_size=0.30, random_state=seed, shuffle=True)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp,test_size=0.50, random_state=seed, shuffle=True)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44001c93-9038-4a6b-91ef-cf21aa4508aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset1_blobs600\n",
      "Shapes: (420, 3) (90, 3) (90, 3)\n",
      "Dataset2_circles500\n",
      "Shapes: (350, 2) (75, 2) (75, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "hidden_sizes = [2, 3, 5, 10, 20]\n",
    "datasets = [(\"Dataset1_blobs600\", r\"../datasets/blobs600.csv\"),(\"Dataset2_circles500\", r\"../datasets/circles500.csv\")]\n",
    "results = {}\n",
    "\n",
    "for name, path in datasets:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    Y = df[\"Class\"].values\n",
    "    del df[\"Class\"]\n",
    "    X = df.values\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = data_split(X, Y, seed=42)\n",
    "\n",
    "    print(name)\n",
    "    print(\"Shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "    results[name] = []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa0223d-b519-481c-993d-9355f2078612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_nodes= 2 | train= 0.5142857142857142 | val= 0.5066666666666667 | test= 0.4266666666666667\n",
      "hidden_nodes= 3 | train= 0.5085714285714286 | val= 0.48 | test= 0.41333333333333333\n",
      "hidden_nodes= 5 | train= 0.5057142857142857 | val= 0.41333333333333333 | test= 0.5066666666666667\n",
      "hidden_nodes= 10 | train= 0.4857142857142857 | val= 0.49333333333333335 | test= 0.5733333333333334\n",
      "hidden_nodes= 20 | train= 0.4857142857142857 | val= 0.49333333333333335 | test= 0.5733333333333334\n"
     ]
    }
   ],
   "source": [
    "for h in hidden_sizes :\n",
    "    lr = LogisticRegressor(alpha=0.01,max_iters=100000,threshold=1e-6,N=1000,hidden_layer_size=h)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    train_acc = lr.score(X_train, y_train)\n",
    "    val_acc = lr.score(X_val, y_val)\n",
    "    test_acc = lr.score(X_test, y_test)\n",
    "\n",
    "    results[name].append((h, train_acc, val_acc, test_acc))\n",
    "\n",
    "    print(\"hidden_nodes=\", h,\"| train=\", train_acc, \"| val=\", val_acc, \"| test=\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbb009-ca18-4919-b843-45fec251d9e0",
   "metadata": {},
   "source": [
    "The current implementation does not achieve high accuracy which indicates that training may not be converging properly.\n",
    "The shallow neural network doesn't clearly outperform logistic regression, while some configurations slightly improve test accuracy, performance is a bit unstable.\n",
    "Increasing the number of hidden layers don't clearly improve the performance but larger configurations show slight improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213ba1b-ef3b-4743-8b16-b2b48ea92126",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
